{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter s(think of temperature)\n",
    "if logits=[1,-1,-1]-->softmax(dim=1)-->[.76,.17,.17]\n",
    "s-->[30,-30,-30]-->softmax(dim=1) \n",
    "\n",
    "\n",
    "LDAM: Label-distribution-aware margin loss.\n",
    "\n",
    "Regularize the minority classes more strongly. l2 regularizer: weight dependent---> data-dependent or label-dependent.\n",
    "\n",
    "Reweighting methods tend to make the optimization of deep models difficult. Reweighting by inverse class frequency yields poor performances on frequent classes.\n",
    "\n",
    "Margin: $$L_{\\text{LDAM}}=\\max(\\max_{j\\neq y}{z_j}-z_y+\\Delta_y,0),$$ where $\\Delta_j=\\frac{C}{n_j^{1/4}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kaidic/LDAM-DRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def focal_loss(input_values, gamma):\n",
    "    \"\"\"Computes the focal loss\"\"\"\n",
    "    p = torch.exp(-input_values)\n",
    "    loss = (1 - p) ** gamma * input_values\n",
    "    return loss.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=0.):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        assert gamma >= 0\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), self.gamma)\n",
    "\n",
    "class LDAMLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, cls_num_list, max_m=0.5, weight=None, s=30):\n",
    "        super(LDAMLoss, self).__init__()\n",
    "        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n",
    "        m_list = m_list * (max_m / np.max(m_list))\n",
    "        m_list = torch.cuda.FloatTensor(m_list)\n",
    "        self.m_list = m_list\n",
    "        assert s > 0\n",
    "        self.s = s\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        index = torch.zeros_like(x, dtype=torch.uint8)\n",
    "        index.scatter_(1, target.data.view(-1, 1), 1) # onehot encoding of label\n",
    "        \n",
    "        index_float = index.type(torch.cuda.FloatTensor)\n",
    "        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n",
    "        batch_m = batch_m.view((-1, 1))\n",
    "        x_m = x - batch_m\n",
    "    \n",
    "        output = torch.where(index, x_m, x)\n",
    "        return F.cross_entropy(self.s*output, target, weight=self.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
