{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer of a neural network has inputs with a corresponding distribution, which is affected during the training process. The effect of these sources of randomness on the distribution of the inputs to internal layers during training is described as internal covariate shift. \n",
    "\n",
    "Reduce internal covariate shift\n",
    "\n",
    "Whitened: linearly transformed to have zero means and unit variances, and decorrelated.\n",
    "\n",
    "Higher learning rate without vanishing or exploding gradients\n",
    "\n",
    "Regularizing effect --> unnecessary to use dropout to mitigate overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attr:`momentum` argument is different from one used in optimizer classes and the conventional notion of momentum.\n",
    "view it as moving avg.\n",
    "\n",
    "Learnable params: \\gamma and \\beta: for each dim of input (can represent the identity transform)\n",
    "if trainable: computes mean and var according to the current batch and updates the running stats\n",
    "else: use running stats for validation\n",
    "\n",
    "Batch Normalization is done over the `C` dimension, computing statistics on `(N, L)` slices.\n",
    "\n",
    "Why over C dim?\n",
    "Because weights shared across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparison of manual BatchNorm2d layer implementation in Python and\n",
    "nn.BatchNorm2d\n",
    "@author: ptrblck\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyBatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=True, track_running_stats=True):\n",
    "        super(MyBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats: \n",
    "            if self.num_batches_tracked is not None:\n",
    "                self.num_batches_tracked += 1\n",
    "                if self.momentum is None:  # use cumulative moving average\n",
    "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "                else:  # use exponential moving average\n",
    "                    exponential_average_factor = self.momentum\n",
    "\n",
    "        # calculate running estimates\n",
    "        if self.training:\n",
    "            mean = input.mean([0, 2, 3])\n",
    "            # use biased var in train\n",
    "            var = input.var([0, 2, 3] , unbiased=False)\n",
    "            n = input.numel() / input.size(1)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = exponential_average_factor * mean\\\n",
    "                    + (1 - exponential_average_factor) * self.running_mean\n",
    "                # update running_var with unbiased var\n",
    "                self.running_var = exponential_average_factor * var * n / (n - 1)\\\n",
    "                    + (1 - exponential_average_factor) * self.running_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
    "        if self.affine:\n",
    "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
    "\n",
    "        return input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
