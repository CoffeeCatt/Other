{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address the class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples.\n",
    "\n",
    "It focuses training on a set of hard examples.\n",
    "\n",
    "$$\\text{FL}(p_t)=-(1-p_t)^\\gamma \\log p_t,$$\n",
    "$\\gamma=0$, $\\text{FL}=\\text{CE}$. \n",
    "Default: $\\gamma=2$.\n",
    "\n",
    "F.cross_entropy--> calculate $-\\log p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kaidic/LDAM-DRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def focal_loss(input_values, gamma):\n",
    "    \"\"\"Computes the focal loss\"\"\"\n",
    "    p = torch.exp(-input_values)\n",
    "    loss = (1 - p) ** gamma * input_values\n",
    "    return loss.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=0.):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        assert gamma >= 0\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), self.gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
